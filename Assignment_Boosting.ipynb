{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is Boosting in Machine Learning?**\n",
        "   Boosting is an ensemble technique that combines weak learners sequentially to create a strong model.\n",
        "\n",
        "2. **How does Boosting differ from Bagging?**\n",
        "   Boosting trains models sequentially to correct errors; Bagging trains in parallel to reduce variance.\n",
        "\n",
        "3. **What is the key idea behind AdaBoost?**\n",
        "   Focus more on previously misclassified samples by adjusting weights.\n",
        "\n",
        "4. **Explain the working of AdaBoost with an example.**\n",
        "   It trains a series of weak learners and increases weight on misclassified data at each step.\n",
        "\n",
        "5. **What is Gradient Boosting, and how is it different from AdaBoost?**\n",
        "   Gradient Boosting optimizes a loss function via gradients, while AdaBoost adjusts sample weights.\n",
        "\n",
        "6. **What is the loss function in Gradient Boosting?**\n",
        "   Commonly used: Mean Squared Error for regression and Log Loss for classification.\n",
        "\n",
        "7. **How does XGBoost improve over traditional Gradient Boosting?**\n",
        "   It includes regularization, parallelization, and handling of missing values for better performance.\n",
        "\n",
        "8. **What is the difference between XGBoost and CatBoost?**\n",
        "   CatBoost is better for categorical features; XGBoost is faster and more mature in ecosystem.\n",
        "\n",
        "9. **What are some real-world applications of Boosting techniques?**\n",
        "   Fraud detection, ranking in search engines, loan risk prediction, and ad click prediction.\n",
        "\n",
        "10. **How does regularization help in XGBoost?**\n",
        "    It reduces overfitting by penalizing complex trees.\n",
        "\n",
        "11. **What are some hyperparameters to tune in Gradient Boosting models?**\n",
        "    Learning rate, number of estimators, max\\_depth, subsample, and loss function.\n",
        "\n",
        "12. **What is the concept of Feature Importance in Boosting?**\n",
        "    It shows how much each feature contributes to the model's predictions.\n",
        "\n",
        "13. **Why is CatBoost efficient for categorical data?**\n",
        "    It handles categorical data internally without needing manual preprocessing.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4RD1tYqtOpwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
        "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor, plot_importance\n",
        "from catboost import CatBoostClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "bc = load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 14. AdaBoost Classifier\n",
        "ada_clf = AdaBoostClassifier()\n",
        "ada_clf.fit(X_train, y_train)\n",
        "print(\"Q14 AdaBoost Accuracy:\", ada_clf.score(X_test, y_test))\n",
        "\n",
        "# 15. AdaBoost Regressor with MAE\n",
        "housing = fetch_california_housing()\n",
        "Xh, yh = housing.data, housing.target\n",
        "Xh_train, Xh_test, yh_train, yh_test = train_test_split(Xh, yh, test_size=0.3, random_state=42)\n",
        "ada_reg = AdaBoostRegressor()\n",
        "ada_reg.fit(Xh_train, yh_train)\n",
        "print(\"Q15 AdaBoost MAE:\", mean_absolute_error(yh_test, ada_reg.predict(Xh_test)))\n",
        "\n",
        "# 16. Gradient Boosting Classifier & feature importance\n",
        "gb_clf = GradientBoostingClassifier()\n",
        "gb_clf.fit(X_train, y_train)\n",
        "print(\"Q16 Accuracy (Gradient Boosting):\", gb_clf.score(X_test, y_test))\n",
        "print(\"Q16 Feature Importance:\", gb_clf.feature_importances_)\n",
        "\n",
        "# 17. Gradient Boosting Regressor & R2\n",
        "gb_reg = GradientBoostingRegressor()\n",
        "gb_reg.fit(Xh_train, yh_train)\n",
        "print(\"Q17 R2 Score:\", r2_score(yh_test, gb_reg.predict(Xh_test)))\n",
        "\n",
        "# 18. XGBoost vs Gradient Boosting\n",
        "xgb_clf = XGBClassifier(eval_metric='logloss')\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "print(\"Q18 XGBoost Accuracy:\", xgb_clf.score(X_test, y_test))\n",
        "print(\"Q18 Gradient Boosting Accuracy:\", gb_clf.score(X_test, y_test))\n",
        "\n",
        "# 19. CatBoost & F1\n",
        "cat_clf = CatBoostClassifier(verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "print(\"Q19 CatBoost F1:\", f1_score(y_test, cat_clf.predict(X_test)))\n",
        "\n",
        "# 20. XGBoost Regressor & MSE\n",
        "xgb_reg = XGBRegressor()\n",
        "xgb_reg.fit(Xh_train, yh_train)\n",
        "print(\"Q20 XGBoost MSE:\", mean_squared_error(yh_test, xgb_reg.predict(Xh_test)))\n",
        "\n",
        "# 21. AdaBoost feature importance\n",
        "print(\"Q21 AdaBoost Feature Importance:\", ada_clf.feature_importances_)\n",
        "\n",
        "# 22. Gradient Boosting loss curves\n",
        "plt.plot(gb_reg.train_score_)\n",
        "plt.title(\"Q22: Gradient Boosting Loss Curve\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "# 23. XGBoost Feature Importance\n",
        "plot_importance(xgb_clf)\n",
        "plt.title(\"Q23: XGBoost Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "# 24. CatBoost Confusion Matrix\n",
        "y_pred_cat = cat_clf.predict(X_test)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_cat)\n",
        "plt.title(\"Q24: CatBoost Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# 25. AdaBoost: compare estimators\n",
        "for n in [10, 50, 100]:\n",
        "    clf = AdaBoostClassifier(n_estimators=n)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(f\"Q25 Accuracy with {n} estimators:\", clf.score(X_test, y_test))\n",
        "\n",
        "# 26. Gradient Boosting ROC Curve\n",
        "probs = gb_clf.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.title(\"Q26: Gradient Boosting ROC\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 27. XGBoost Regressor GridSearchCV\n",
        "params = {'n_estimators': [50, 100], 'learning_rate': [0.05, 0.1]}\n",
        "grid = GridSearchCV(XGBRegressor(), param_grid=params, cv=3)\n",
        "grid.fit(Xh_train, yh_train)\n",
        "print(\"Q27 XGBoost Best Params:\", grid.best_params_)\n",
        "\n",
        "# 28. CatBoost imbalance performance\n",
        "# Simulate imbalance by taking 90% of class 0, 10% of class 1\n",
        "idx_0 = np.where(y == 0)[0]\n",
        "idx_1 = np.where(y == 1)[0]\n",
        "imb_idx = np.concatenate([idx_0[:int(0.9*len(idx_0))], idx_1[:int(0.1*len(idx_1))]])\n",
        "X_imb, y_imb = X[imb_idx], y[imb_idx]\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(X_imb, y_imb, test_size=0.3, random_state=42)\n",
        "cat_imb = CatBoostClassifier(verbose=0, class_weights=[1, 10])\n",
        "cat_imb.fit(X_train_imb, y_train_imb)\n",
        "print(\"Q28 CatBoost Imbalanced F1:\", f1_score(y_test_imb, cat_imb.predict(X_test_imb)))\n",
        "\n",
        "# 29. AdaBoost log loss\n",
        "from sklearn.metrics import log_loss\n",
        "print(\"Q29 AdaBoost Log Loss:\", log_loss(y_test, ada_clf.predict_proba(X_test)))\n",
        "\n",
        "# 30. Gradient Boosting Log Loss\n",
        "print(\"Q30 Gradient Boosting Log Loss:\", log_loss(y_test, gb_clf.predict_proba(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "_zE0kfNlO4F8",
        "outputId": "088535ca-8cf6-485d-b06d-7287004446db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-efe8956a876e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}